In this project I want to Bechmark Models and optimize them on multiple hardwares
The first Model I'll be trying is the yolov12n (mainly since it contains many primitive ops i want to try to benchmark and optimize , conv matmul , many activations and resizing and reshaping and contains also an attention layer)

the hardwares I'll be testing on are these , in this order:
digital ocean VM featuring an intel Xeon 2 vCPUs and 2 GB of RAM and good amount of SSD , ubuntu 25.x OS
avnet ultra96v2 board 
Jetson Nano developer kit with 2GB VRAM GPU

what i wanna do first is make a setup.sh file and put it in the github repo that would make it work regardless of which platform (of the mentioned 3)
it is deployed on, i want the highlevel to be correct for all the 3 platforms , but i want to be especially correct and working for the digital ocean's VM , sicne i'll be using it first , the rest I'll fix iteratively
so what i want is to once i get into the terminal of each machine and git clone my repo, setup.sh installs all the required dependencies ( from libraries and runtimes to git lfs my onnx models to testing for ) and makes the environment suitable and perfect for running tests to see if the hardware is working correctly to printing hardware specifications .....
but i want u to start iteratively , just put placeholders in the avnet and the jetson part.
my models are in onnx format and i'll be using onnx cpp api and onnxruntime of cpp , it needs to be installed depending on the hardware too
so make a setup.sh -hw {hardware type} argument in the cli

here's a normal cpu code that works on my machine 
{
#include <onnxruntime_cxx_api.h>
#include <vector>
#include <random>
#include <chrono>
#include <iostream>

int main() {
    // ----------------------------
    // 1️⃣ Initialize ONNX Runtime
    // ----------------------------
    Ort::Env env(ORT_LOGGING_LEVEL_WARNING, "benchmark");
    Ort::SessionOptions options;
    options.SetIntraOpNumThreads(4); // CPU threads
    options.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_ENABLE_ALL);
    options.EnableProfiling("onnxruntime_profile.json"); // Enable per-node profiling

    std::string model_path = "/mnt/USEFUL/pro/GetCracked/REPOs/MetalBench/Models/yolo12n.onnx";
    Ort::Session session(env, model_path.c_str(), options);

    // ----------------------------
    // 2️⃣ Prepare input tensor
    // ----------------------------
    auto input_shape = session.GetInputTypeInfo(0).GetTensorTypeAndShapeInfo().GetShape();
    for (auto &d : input_shape)
        if (d <= 0) d = 1; // dynamic dims -> 1

    size_t tensor_size = 1;
    for (auto d : input_shape)
        tensor_size *= d;

    std::vector<float> input_data(tensor_size);
    std::mt19937 gen(42);
    std::uniform_real_distribution<float> dist(0.f, 1.f);
    for (auto &x : input_data)
        x = dist(gen);

    Ort::MemoryInfo mem_info = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);
    Ort::Value input_tensor = Ort::Value::CreateTensor<float>(
        mem_info, input_data.data(), tensor_size, input_shape.data(), input_shape.size()
    );

    // Input/output names
    std::vector<std::string> input_names = session.GetInputNames();
    std::vector<std::string> output_names = session.GetOutputNames();
    std::vector<const char*> input_names_c, output_names_c;
    for (auto &s : input_names) input_names_c.push_back(s.c_str());
    for (auto &s : output_names) output_names_c.push_back(s.c_str());

    // ----------------------------
    // 3️⃣ Warm-up runs
    // ----------------------------
    const int warmup_runs = 5;
    for (int i = 0; i < warmup_runs; ++i) {
        session.Run(
            Ort::RunOptions{nullptr},
            input_names_c.data(), &input_tensor, 1,
            output_names_c.data(), output_names_c.size()
        );
    }

    // ----------------------------
    // 4️⃣ Benchmark loop
    // ----------------------------
    const int num_runs = 50;
    std::vector<double> times;
    times.reserve(num_runs);

    for (int i = 0; i < num_runs; ++i) {
        auto t1 = std::chrono::high_resolution_clock::now();

        auto output_tensors = session.Run(
            Ort::RunOptions{nullptr},
            input_names_c.data(), &input_tensor, 1,
            output_names_c.data(), output_names_c.size()
        );

        auto t2 = std::chrono::high_resolution_clock::now();
        std::chrono::duration<double, std::milli> duration = t2 - t1;
        times.push_back(duration.count());
    }

    // ----------------------------
    // 5️⃣ Compute statistics
    // ----------------------------
    double sum = 0.0, min_time = times[0], max_time = times[0];
    for (auto t : times) {
        sum += t;
        if (t < min_time) min_time = t;
        if (t > max_time) max_time = t;
    }
    double avg_latency = sum / times.size();

    std::cout << "Benchmark results over " << num_runs << " runs:" << std::endl;
    std::cout << "Average latency: " << avg_latency << " ms" << std::endl;
    std::cout << "Min latency: " << min_time << " ms" << std::endl;
    std::cout << "Max latency: " << max_time << " ms" << std::endl;
    std::cout << "Throughput: " << 1000.0 / avg_latency << " FPS" << std::endl;
    std::cout << "Per-node profiling saved to 'onnxruntime_profile.json'" << std::endl;

    return 0;
}

}

here's my repo structure attached in the image